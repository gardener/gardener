#!/bin/bash -e

readonly own_dir="$(dirname "$(readlink -f "$0")")"
readonly template_dir="${own_dir}/templates-it"
source "${own_dir}/lib/common"

# -------------------------
# | Arguments / Constants |
# -------------------------

# Script arguments
cluster_only=false
garden_cluster_kubeconfig=""
seed_cluster_kubeconfig=""
shoot_cluster_kubeconfig=""
watch_namespace="garden"
gardener_namespace="garden"
shoot_cluster_name_prefix="it"
shoot_cluster_name_suffix="$(printf '%x' "$(date +%s)" | cut -c 5-)"
wait_for_cluster_deletion=false
shoot_manifest_template="${own_dir}/../hack/templates/resources/shoot.yaml.tpl"
rc_cleanup_error=100
minimal_log_level="warning"
ci_cli=""
kubectl_binary_path="kubectl"
helm_binary_path="helm"

# Script variables
# busybox' output for 'date -Iseconds' does not contain a colon character in TZ offset (fix with sed)
start_time_rfc3339="$(date -Iseconds | sed -E -e 's/(.*)([0-9]{2}):{0,1}([0-9]{2})$/\1\2:\3/')"
shoot_cluster_name="" # filled with "${shoot_cluster_name_prefix}${shoot_cluster_name_suffix}" later
shoot_domain_suffix=""
seed_cluster_name=""
shoot_spec=""
cloud_provider=""
garden_controller_manager_name=""
# Used to determine whether there is some cleaning-up to do
test_execution_was_successful=false

# Script constants
readonly current_dir="$PWD"
readonly tmp_dir="${own_dir}/tmp"
readonly logs_path="${own_dir}/.logs"
readonly seed_secrets_ns="garden" # always garden by convention
readonly helm_deploy_ns="kube-system" # should match namespace given in the guestbook-app yaml

# ------------------
# | Test Functions |
# ------------------

# HTTP GET
function http_get() {
  args_before_last=${@:1:$(($#-1))}
  args_only_last="${@:$#}"

  if ! response="$(curl $args_before_last -sfkL "$args_only_last")"; then
    # repeat the curl invocation without the 'silent' option in order to
    # retrieve error output in case those are of interest (e.g. for debugging)
    response="$(curl $args_before_last -kLi "$args_only_last")" || true # keep going
    echo -e "$(error "${response}")"
    return 1
  fi
  echo "${response}"
}

# Returns the name of the namespace on the seed cluster hosting the control plane of the shoot
function determine_shoot_namespace() {
  local shoot_namespace=$(echo ${watch_namespace} | cut -d- -f 2)
  echo "shoot-${shoot_namespace}-${shoot_cluster_name}"
}

# Download kubeconfig for Seed cluster
function download_seed_kubeconfig() {
  local seed=$(exec_kubectl "${garden_cluster_kubeconfig}" get seed $seed_cluster_name -o json)
  local seed_secret_name="$(echo "$seed" | jq -r .spec.secretRef.name)"
  local seed_secret_ns="$(echo "$seed" | jq -r .spec.secretRef.namespace)"
  local secret="$(exec_kubectl "${garden_cluster_kubeconfig}" --namespace="$seed_secret_ns" get secret $seed_secret_name -o json)" # get secret first (don't pipe with jq), so that function aborts if exec_kubectl fails (pipe would hide failure)
  echo "$secret" | jq -r '.data.kubeconfig' | base64 -d > "${own_dir}/kubeconfig-$seed_cluster_name.yaml"
  seed_cluster_kubeconfig="${own_dir}/kubeconfig-$seed_cluster_name.yaml"
}

# Download kubeconfig for Shoot cluster
function download_shoot_kubeconfig() {
  local shoot_namespace="$(determine_shoot_namespace)"
  echo "$shoot_namespace"
  secret="$(exec_kubectl "${seed_cluster_kubeconfig}" --namespace=${shoot_namespace} get secret kubecfg -o json)" # get secret first (don't pipe with jq), so that function aborts if exec_kubectl fails (pipe would hide failure)
  echo "$secret" | jq -r '.data.kubeconfig' | base64 -d > "${own_dir}/kubeconfig-$shoot_cluster_name.yaml"
  shoot_cluster_kubeconfig="${own_dir}/kubeconfig-$shoot_cluster_name.yaml"
}

# Read admin password
function get_admin_password() {
  local shoot_namespace="$(determine_shoot_namespace)"
  secret="$(exec_kubectl "${seed_cluster_kubeconfig}" --namespace=${shoot_namespace} get secret kubecfg -o json)" # get secret first (don't pipe with jq), so that function aborts if exec_kubectl fails (pipe would hide failure)
  echo "$secret" | jq -r '.data.password' | base64 -d
}


function delete_kubeconfig() {
  if ! ensure_file_exists "${1}"; then
    error "provided kubeconfig file ${1} does not exist"
    return 1
  fi
  rm -f "${1}"
}

function ensure_file_exists() {
  if [[ ! -f "${1}" ]]; then
    error "file does not exist in expeced location: ${1}"
    return 1
  fi
}

# Run kubectl command using specified kubeconfig
# kubeconfig file $1 must exist
function exec_kubectl() {
  if ! ensure_file_exists "${1}"; then
    error "provided kubeconfig file ${1} does not exist"
    return 1
  fi
  "${kubectl_binary_path}" --kubeconfig "${1}" "${@:2}"
}

# Run helm command
# kubeconfig file $1 must exist
function exec_helm() {
  if ! ensure_file_exists "${1}"; then
    error "provided kubeconfig file ${1} does not exist"
    return 1
  fi
  KUBECONFIG="${1}" "${helm_binary_path}" "${@:2}"
}

# Create cluster
function create_shoot_cluster() {
  # Get infrastructure kind from shoot cluster template and find out where we have a seed cluster running (pick first region) before we can insert that information and can actually create the cluster
  ensure_file_exists "${shoot_manifest_template}"
  shoot_manifest_template_rendered="$(mako-render ${shoot_manifest_template} --var cloud="$cloud_provider" --var values=<(cat <<EOF
metadata:
  name: "${shoot_cluster_name}"
  namespace: "${watch_namespace}"
spec:
  dns:
    domain: "$shoot_dns_name_suffix"
$(cat "$shoot_spec")
EOF
))"
  echo "$shoot_manifest_template_rendered" | exec_kubectl "${garden_cluster_kubeconfig}" --namespace=${watch_namespace} create -f -
}

# Delete cluster
function delete_shoot_cluster() {
  exec_kubectl "${garden_cluster_kubeconfig}" --namespace "${watch_namespace}" delete shoot "${shoot_cluster_name}"
  deletionTimestamp="$(exec_kubectl "${garden_cluster_kubeconfig}" --namespace "${watch_namespace}" get shoot "${shoot_cluster_name}" -o jsonpath --template={.metadata.deletionTimestamp})"
  exec_kubectl "${garden_cluster_kubeconfig}" --namespace "${watch_namespace}" patch shoot "${shoot_cluster_name}" --type=merge -p "{\"metadata\": {\"annotations\": {\"confirmation.garden.sapcloud.io/deletionTimestamp\": \"$deletionTimestamp\"}}}"
}

# Get cluster state
function get_cluster_state() {
  if ! cluster_resource="$(exec_kubectl "${garden_cluster_kubeconfig}" --namespace=${watch_namespace} get shoot $shoot_cluster_name -o json 2>&1)"; then
    if [[ "$cluster_resource" == "Error from server (NotFound)"* ]]; then
      cluster_resource='{"status":{"lastOperation":{"type":"Delete", "state": "Completed"}}}';
    else
      cluster_resource='{"status":{"lastOperation":{"type":"Unknown", "state": "Error"}}}';
    fi
  fi
  cluster_state="$(echo "${cluster_resource}" | jq -r '.status.lastOperation.type + " " + .status.lastOperation.state' 2> /dev/null|| echo "Unknown")"
  case $cluster_state in
    "Create Succeeded"|"Update Succeeded"|"Create Failed"|"Update Failed"|"Delete Failed"|"Delete Completed"|"Delete Succeeded")
      cluster_state="$(echo "${cluster_state}" | tr /a-z/ /A-Z/) [Final]";;
    *)
      cluster_state="$(echo "${cluster_state}" | tr /a-z/ /A-Z/) [In Progress]";;
  esac
  echo "${cluster_state}"
}


# Get pod health
function get_pod_health() {
  pods="$(exec_kubectl "${shoot_cluster_kubeconfig}" get pods --all-namespaces 2> /dev/null)" # get pods first (don't pipe with grep), so that function aborts if exec_kubectl fails (pipe would hide failure)
  echo "$pods" | (grep -E "$1" || echo "UNKNOWN UNKNOWN 0/0 UNKNOWN") | awk '{print toupper($4)" ("$1"/"$2"#"$3")"}'
}

# Wait for desired cluster state
# $1: the desired cluster state (mandatory)
# $2: the current cluster state (optional)
# In the special case where out cluster already is in a final state, but we perform
# asynchronous operations that intend to change this state, a race condition may
# occur, where this function encounters the old (final) state and thus decides it is
# not valid to wait for a state change. To properly handle those cases, optionally
# pass the previous cluster state; for this particulat state, said shortcut will not
# be applied.
function wait_for_desired_cluster_state() {
  desired_cluster_state="$1"
  previous_cluster_state="${2:-}"
  max_wait_time=2700
  optype="$(echo "${desired_cluster_state}" | cut -d' ' -f1)"

  if "${ci_cli}" kubeutil --kubeconfig  "${garden_cluster_kubeconfig}" \
    wait_for_shoot_cluster_operation_success \
    --namespace "${watch_namespace}" \
    --shoot-name "${shoot_cluster_name}" \
    --optype  "${optype}" \
    --timeout-seconds "${max_wait_time}" \
    ; then
    echo -e "$(ok Cluster reached desired state)"
    return
  else
    if [[ -f "${own_dir}/$shoot_cluster_kubeconfig" ]]; then
      set +e
      exec_kubectl "${shoot_cluster_kubeconfig}" get nodes
      exec_kubectl "${shoot_cluster_kubeconfig}" get pods --all-namespaces
      set -e
    fi
    fail "Cluster did not reach desired state"
  fi
}

# Wait for ready cluster state
function wait_for_ready_cluster_state() {
  if "${ci_cli}" kubeutil --kubeconfig "${garden_cluster_kubeconfig}" \
    wait_for_shoot_cluster_to_become_healthy \
    --namespace "${watch_namespace}" \
    --shoot-name "${shoot_cluster_name}" \
    --timeout-seconds 1200; then
    echo -e "$(ok Cluster became healthy)"
    return
  else
    fail "Cluster did not become healthy"
  fi
}

# Wait for running pod state
function wait_for_running_pod_state() {
  max_wait_time=1200
  wait_stop=$(($(date +%s) + max_wait_time))
  pod_health="$(get_pod_health "$1")"
  while [[ "$pod_health" != "RUNNING"* ]] && [[ $(date +%s) -lt $wait_stop ]]; do
    echo -e "$(debug Pod is "${pod_health}". Waiting...)"
    sleep 5
    pod_health="$(get_pod_health "$1")"
  done
  if [[ "$pod_health" == "RUNNING"* ]]; then
    echo -e "$(ok Pod reached running state $pod_health.)"
  else
    set +e
    exec_kubectl "${shoot_cluster_kubeconfig}" get nodes
    exec_kubectl "${shoot_cluster_kubeconfig}" get pods --all-namespaces | grep -E "$1"
    set -e
    echo -e "$(error Pod reached no running state within $max_wait_time seconds.)"
    return 1
  fi
}

# Determine Seed cluster name
function determine_seed_cluster_name() {
  cluster_resource="$(exec_kubectl "${garden_cluster_kubeconfig}" --namespace=${watch_namespace} get shoot $shoot_cluster_name -o json)"
  seed_cluster_name="$(echo "${cluster_resource}" | jq -r '.spec.cloud.seed')"
}

function determine_garden_controller_manager_name() {
  garden_controller_manager_name=$(exec_kubectl "${garden_cluster_kubeconfig}" --namespace=${gardener_namespace} get pods --selector=app=gardener,role=controller-manager -o jsonpath='{.items[*].metadata.name}')
  echo "${garden_controller_manager_name}"
}

# Wait for garden controller-manager pod to start-up
# This is relevant for integration test pipeline scenarios, where the garden controller-manager
# is being deployed immediately before executing the integration tests. In this case, it
# is necessary to wait until the garden-controller-manager pod becomes operational.
function wait_for_garden_controller_manager_pod() {
  max_wait_time=60 # seconds
  wait_stop=$(($(date +%s) + max_wait_time))
  while [[ $(date +%s) -lt ${wait_stop} ]] ; do
    if ! state="$(exec_kubectl "${garden_cluster_kubeconfig}" --namespace=${gardener_namespace} get pod $(determine_garden_controller_manager_name) -o jsonpath='{.status.phase}')"; then
      echo -e "$(warning "received an error from kubectl trying to determine garden-controller-manager pod state")"
    fi
    state_uppercase="$(echo ${state} | awk '{print toupper($1)}')"
    if [[ "${state_uppercase}" == "RUNNING" ]]; then
      # controller-manager pod reached operational state
      break;
    else
      echo -e "$(info "garden-controller-manager-pod has not yet started, yet (${state_uppercase})")"
      sleep 10
    fi
  done
}

# Download controller-manager logs
function download_controller_manager_logs() {
  mkdir -p "${logs_path}" &> /dev/null
  logfile="${logs_path}/garden-controller-manager.log"
  garden_controller_manager_name=$(determine_garden_controller_manager_name)

  if ! "${ci_cli}" kubeutil --kubeconfig "${garden_cluster_kubeconfig}" \
    retrieve_controller_manager_log_entries \
    --pod-name "${garden_controller_manager_name}" \
    --namespace "${gardener_namespace}" \
    --only-if-newer-than-rfc3339-ts "${start_time_rfc3339}" \
    --filter-for-shoot-name "${shoot_cluster_name}" \
    --minimal-loglevel "${minimal_log_level}" > "${logfile}" ; then
  echo -e "$(error "received an error after trying to retrieve controller manager logs")"
  fi

  issues="$(cat "${logfile}" | grep -E ' level=(warning|error|fatal) ' || true)"
  if [[ ! -z "${issues}" ]]; then
    echo -e "$(error The controller manager logs contain the following issues:)\n$issues\n"
  fi
}

# Wait for DNS entry to be propagated
# $1: dns entry to wait for
function wait_for_dns_entry() {
  dns_entry="${1}"

  start_time="$(date +%s)"
  timeout=300 # 300 seconds = 5 minutes
  timeout_time=$((start_time + timeout))
  while ! _="$(nslookup "${dns_entry}" 2>&1)"; do
    echo -e "$(info "Waiting for DNS entry "${dns_entry}" to be propagated")"
    sleep 10
    now="$(date +%s)"
    if [[ ${now} -gt ${timeout_time} ]]; then
      fail "DNS entry ${dns_entry} was not propagated after ${timeout} seconds"
      break
    fi
  done
}

# ---------------------
# | Utility Functions |
# ---------------------

# Ensure that this script's runtime dependencies are met, fail early if not
function check_runtime_dependencies() {
  required_executables=(jq helm kubectl mako-render pip3)
  for executable in "${required_executables[@]}"; do
    if ! _="$(which ${executable})" ; then
      fail "${executable} not found in PATH"
    fi
  done
  # initialise helm (if helm is not intialised, helm charts are not looked up)
  _=$(helm init --client-only)
}

function validate_command_line_arguments() {
  if [[ -z "${garden_cluster_kubeconfig}" ]]; then
    fail "no kubeconfig file provided"
  else
    set +e
    ensure_file_exists "${garden_cluster_kubeconfig}"
    if [[ $? != 0 ]]; then
      fail "Garden cluster kubeconfig does not exist on provided path ${garden_cluster_kubeconfig}"
    fi
    set -e
  fi

  if [[ -z "$cloud_provider" ]]; then
    fail "No cloud provider provided"
  fi

  if [[ -z "$shoot_domain_suffix" ]]; then
    fail "No shoot domain suffix specified"
  fi
}

function retrieve_ci_utils_if_absent() {
  # if --ci-cli option was given, verify it points to an existing file
  if [[ ! -z "${ci_cli}" ]]; then
    set +e
    ensure_file_exists "${ci_cli}"
    if [[ $? != 0 ]]; then
      fail "cc-utils project does not exists on provided path"
    fi
    set -e
    return
  fi

  # fallback: if --ci-cli was not specified, clone the repository
  ci_utils_dir="${own_dir}/ci-utils"
  ci_cli="${ci_utils_dir}/cli.py"
  if [ ! -d "${ci_utils_dir}" ]; then
    GIT_SSL_NO_VERIFY=1 git clone https://github.com/gardener/cc-utils "${ci_utils_dir}"
  else
    GIT_WORK_TREE="${ci_utils_dir}" GIT_DIR="${ci_utils_dir}/.git" git pull -f --rebase origin
  fi
  pip3 install -r "${ci_utils_dir}/requirements.txt" &> /dev/null
}

# ------------------
# | Main Functions |
# ------------------

# Clean up resources if requested and echo summary
function end_execution() {
  # Store result and delete trap handler to avoid recursion
  result=$?
  trap - HUP INT QUIT PIPE TERM EXIT

  if [[ $result != 0 ]]; then
    echo -e "$(warning An error was encountered - cleaning up resources)"
  fi

  # Download logs after the integration tests complete
  download_controller_manager_logs || true

  # Delete resources (if there is still something to be deleted)
  if ! "${test_execution_was_successful}"; then
    set +e
    echo -e "$(info Deleting redis helm chart and its resources...)"
    exec_kubectl "${shoot_cluster_kubeconfig}" delete -n "${helm_deploy_ns}" ingress guestbook &> /dev/null
    exec_kubectl "${shoot_cluster_kubeconfig}" delete -n "${helm_deploy_ns}" service guestbook &> /dev/null
    exec_kubectl "${shoot_cluster_kubeconfig}" delete -n "${helm_deploy_ns}" deployment guestbook &> /dev/null
    exec_helm "${shoot_cluster_kubeconfig}" delete --purge redis &> /dev/null
    echo -e "$(info Trigger shoot deletion...)"
    delete_shoot_cluster
    if ${wait_for_cluster_deletion}; then
      # only wait for deletion if the shoot exits
      "${kubectl_binary_path}" --kubeconfig "${garden_cluster_kubeconfig}" -n "${watch_namespace}" get shoot "${shoot_cluster_name}" &> /dev/null
      if [[ $? -eq 0 ]]; then
        wait_for_desired_cluster_state "DELETE"
        if [[ ! $? -eq 0 ]]; then
          echo -e "\n(error FAILURE:) Could not delete all resources - manual cleanup required"
          result="${rc_cleanup_error}"
        fi
      fi
    fi
    echo -e "$(info Delete Shoot and Seed kubeconfigs...)"
    delete_kubeconfig "$shoot_cluster_kubeconfig" &> /dev/null
    delete_kubeconfig "$seed_cluster_kubeconfig" &> /dev/null
    unset shoot_cluster_name # indicates that nothing anymore needs to be deleted
    set -e
  fi

  # Return to former directory
  cd "$current_dir"

  # Inform user about result
  if [[ $result -eq 0 ]]; then
    echo -e "\n$(ok SUCCESS:) Test completed successfully at $(date +"%Y-%m-%dT%T")."
    exit 0
  elif [[ $result -eq 100 ]]; then
    echo -e "\n$(warning FAILURE:) Test aborted at $(date +"%Y-%m-%dT%T")!"
    exit $result
  else
    echo -e "\n$(error FAILURE:) Test failed at $(date +"%Y-%m-%dT%T") (see above for concrete issue)!"
    exit $result
  fi
}

# Test run
function execute() {
  check_runtime_dependencies
  validate_command_line_arguments
  retrieve_ci_utils_if_absent

  shoot_cluster_name="${shoot_cluster_name_prefix}${shoot_cluster_name_suffix}${cloud_provider}"

  # Sanity check: the sum of shoot cluster name and watch namespace lengths must not exceed 24 characters, as they
  # are concatenated to DNS entries, which impose a length restriction
  if [[ $(( ${#shoot_cluster_name} + ${#watch_namespace} )) -gt 23 ]]; then
    fail "The concatenated length of shoot cluster name and watch namespace exceeds the allowed 23 characters"
  fi
  shoot_dns_name_suffix="${shoot_cluster_name}.${watch_namespace}.${shoot_domain_suffix}"

  # Inform user about start
  echo -e "\n$(info Starting integration test run at "${start_time_rfc3339}"...)\n"

  # Install trap handler for proper cleanup, summary and result code
  trap "exit 100" HUP INT QUIT PIPE TERM && trap end_execution EXIT

  # In case garden controller-manager pod has just been deployed (e.g. in integration test pipeline), we
  # have to wait for the garden controller-manager pod to become operational
  wait_for_garden_controller_manager_pod
  garden_controller_manager_name="$(determine_garden_controller_manager_name)"
  echo -e "$(info garden controller-manager name: "${garden_controller_manager_name}")"

  # Initialize logs folder
  rm -rf "$logs_path" &> /dev/null

  # Create cluster
  create_shoot_cluster
  wait_for_desired_cluster_state "CREATE"
  determine_seed_cluster_name
  download_seed_kubeconfig
  download_shoot_kubeconfig

  wait_for_ready_cluster_state

  # It may take some time until DNS entry is propagated
  wait_for_dns_entry "api.${shoot_dns_name_suffix}"

  # Check dashboard (must be available right away)
  echo -e "$(debug Checking dashboard...)"
  if output=$(http_get "-u" "admin:$(get_admin_password)" "https://api.${shoot_dns_name_suffix}/ui" 2>&1); then
    echo -e "$(ok Dashboard test passed!)"
  else
    # Dashboard shortcut "/ui" is deprecated, try to use the full proxy url instead
    if output=$(http_get "-u" "admin:$(get_admin_password)" "https://api.${shoot_dns_name_suffix}/api/v1/namespaces/kube-system/services/https:kubernetes-dashboard:/proxy" 2>&1); then
      echo -e "$(ok Dashboard test passed!)"
    else
      echo -e "$(error Dashboard test failed!)"
      echo -e "$(error Error details: ${output})"
      return 1
    fi
  fi

  # Check monocular (implicitly checking also tiller and nginx)
  echo "$(debug Checking Monocular...)"
  max_retry_time=300 # we need to wait for some time before nginx detects the healthy monocular pod and starts serving requests to it
  retry_stop=$(($(date +%s) + max_retry_time))
  local monocular_api_base_url="https://monocular.ingress.${shoot_dns_name_suffix}/api/v1"
  echo -e "$(debug Fetching repository list)"
  monocular_success=false
  while [[ $(date +%s) -lt $retry_stop ]]; do
    if repo_list="$(http_get "-u" "admin:$(get_admin_password)" "${monocular_api_base_url}/repos")"; then
      monocular_success=true
      break;
    fi
    echo -e "$(debug Monocular not reachable. Waiting...)"
    sleep 5
  done
  if ! $monocular_success; then
    fail "Could not connect to monocular (${repo_list})"
  fi
  if [[ -z "$repo_list" ]]; then
    fail "$(error Monocular returned an empty response)"
  fi
  echo -e "$(debug Fetched repository list)"
  op_succeeded=true
  if ! repo_names="$(echo "$repo_list" | jq -r '.data[].attributes.name')"; then
    echo -e "$(error Received an invalid response from  "${monocular_api_base_url}/repos")"
    echo -e "$(error Response was: "${repo_list}")"
    return 1
  fi
  for repo_name in $repo_names; do
    max_retry_time=300 # we need to wait for some time before nginx detects the healthy monocular pod and starts serving requests to it
    retry_stop=$(($(date +%s) + max_retry_time))
    while ! chart_list="$(http_get "-u" "admin:$(get_admin_password)" "${monocular_api_base_url}/charts/${repo_name}")" && [[ $(date +%s) -lt ${retry_stop} ]]; do
      echo -e "$(debug Monocular repo "${repo_name}" not found. Waiting...)"
      sleep 5
    done
    if [[ -z "$chart_list" ]]; then
      echo -e "$(error Monocular test failed!)"
      return 1
    fi
    if ! chart_count="$(echo "$chart_list" | jq '.data | length')"; then
      echo -e "$(error Received an invalid JSON document as char_list)"
      echo -e "$(error This is what I was sent: "${chart_list}")"
    fi
    if [[ $chart_count -gt 0 ]]; then
      echo -e "$(ok "${chart_count}" charts found in repo "${repo_name}")"
    else
      echo -e "$(error No charts found in repo $repo_name!)"
      op_succeeded=false
    fi
  done
  if ! $op_succeeded; then
    echo -e "$(error Monocular test failed!)"
    return 1
  fi

  # Check whether we should only test the cluster creation/deletion
  if ! ${cluster_only}; then
    # Create service
    max_retry_time=1200
    retry_stop=$(($(date +%s) + max_retry_time))
    exec_helm "${shoot_cluster_kubeconfig}" init --client-only > /dev/null
    exec_helm "${shoot_cluster_kubeconfig}" install -n redis --namespace "${helm_deploy_ns}" stable/redis > /dev/null
    helm_response="$(exec_helm "${shoot_cluster_kubeconfig}" status redis 2>&1 || true)"
    while [[ "$helm_response" != *"STATUS: DEPLOYED"* ]] && [[ $(date +%s) -lt $retry_stop ]]; do
      echo -e "$(debug Redis deployment not ready. Waiting...)"
      sleep 5
      helm_response="$(exec_helm "${shoot_cluster_kubeconfig}" status redis 2>&1 || true)"
    done

    if [[ "$helm_response" != *"STATUS: DEPLOYED"* ]]; then
      echo -e "$(error Redis deployment failed: "${helm_response}")"
      return 1
    fi
    echo -e "$(ok Redis chart deployed)"
    wait_for_running_pod_state 'redis-master-.*'

    # Create application
    guestbook_manifests="$(mako-render --var namespace=${helm_deploy_ns} --var shootDnsHost=${shoot_dns_name_suffix} ${template_dir}/guestbook-app.yaml.tpl)"
    echo "$guestbook_manifests" | exec_kubectl "${shoot_cluster_kubeconfig}" --namespace=${helm_deploy_ns} create -f -
    echo -e "$(ok Guestbook app deployed: http://guestbook.ingress."${shoot_dns_name_suffix}")"

    # Test application
    max_retry_time=1200 # we need to wait for some time before the pods are operational and the service/ingress route is operational
    retry_stop=$(($(date +%s) + max_retry_time))
    while ! http_get "http://guestbook.ingress.${shoot_dns_name_suffix}/lrange/guestbook" &> /dev/null && [[ $(date +%s) -lt $retry_stop ]]; do
      echo -e "$(debug Guestbook not reachable. Waiting...)"
      sleep 5
    done
    if [[ $(date +%s) -lt $retry_stop ]]; then
      echo -e "$(ok Guestbook app available now!)"
    else
      set +e
      exec_kubectl "${shoot_cluster_kubeconfig}" get pods | grep -E "redis|guestbook"
      http_get -v "http://guestbook.ingress.${shoot_dns_name_suffix}/lrange/guestbook"; echo
      set -e
      echo -e "$(error Guestbook deployment failed!)"
      return 1
    fi
    guestbook_response="$(http_get "http://guestbook.ingress.${shoot_dns_name_suffix}/rpush/guestbook/foobar-$shoot_cluster_name")"
    guestbook_response="$(http_get "http://guestbook.ingress.${shoot_dns_name_suffix}/lrange/guestbook")"
    if [[ "$guestbook_response" == *"foobar-$shoot_cluster_name"* ]]; then
      echo -e "$(ok Guestbook app test passed!)"
    else
      set +e
      exec_kubectl "${shoot_cluster_kubeconfig}" get pods | grep -E "redis|guestbook"
      http_get -v "http://guestbook.ingress.${shoot_dns_name_suffix}/lrange/guestbook"; echo
      set -e
      echo -e "$(error Guestbook app test failed!)"
      return 1
    fi

    # Delete application
    exec_kubectl "${shoot_cluster_kubeconfig}" delete -n "${helm_deploy_ns}" ingress guestbook
    exec_kubectl "${shoot_cluster_kubeconfig}" delete -n "${helm_deploy_ns}" service guestbook
    exec_kubectl "${shoot_cluster_kubeconfig}" delete -n "${helm_deploy_ns}" deployment guestbook

    # Delete service
    exec_helm "${shoot_cluster_kubeconfig}" delete --purge redis
  fi

  # Delete cluster
  delete_shoot_cluster
  wait_for_desired_cluster_state "DELETE"
  delete_kubeconfig "$shoot_cluster_kubeconfig"
  delete_kubeconfig "$seed_cluster_kubeconfig"
  test_execution_was_successful=true # indicates that nothing anymore needs to be deleted
}

# Show help
function help() {
  echo -e "Usage: $(basename $0)"
  echo -e "       [-h]. . . . . . . . . . . . . . . . . . . . . . Show this help."
  echo -e "       [-c]. . . . . . . . . . . . . . . . . . . . . . Test only cluster creation and deletion,
                                                       but no application/service.
                                                       DEFAULT: no update"
  echo -e "       [--ci-cli <cli.py>] . . . . . . . . . . . . . . Specify ci-cli executable to use
                                                       This should point to cli.py from repository
                                                       github.com/gardener/cc-utils"
  echo -e "       [-k <path>|--garden-cluster-kubeconfig <path>]  Path to Garden cluster kubeconfig"
  echo -e "       [-d <domain>|--domain-suffix <domain>]. . . . . Domain suffix on a Shoot.
                                                       NOTE: Shoot domain format: <shoot-name>.project-name>.shoot.<domain>"
  echo -e "       [--project-name <ns>|-n <ns>] . . . . . . . . . Project name (namespace in garden cluster into which
                                                       shoot-cluster resources are to be deployed
                                                       NOTE: must contain infrastructure secrets"
  echo -e "       [--gardener-namespace <ns>] . . . . . . . . . . Namespace in which the Gardener runs.
                                                       DEFAULT: garden"
  echo -e "       [--rc-on-cleanup-error <rc>]. . . . . . . . . . Return code to be returned if resources could not be cleaned up.
                                                       DEFAULT: 100, MUST be an integer value"
  echo -e "       [--minimal-log-level <level>] . . . . . . . . . Set the minimal log level used to filter the garden-controller-manager
                                                       log output. Possible values are, in ascending Order: Debug, Info,
                                                       Warning, Error, Fatal and Panic.
                                                       DEFAULT: Warning"
  echo -e "       [-s <name>|--shoot-cluster-name-suffix <name>]  Shoot cluster name suffix.
                                                       DEFAULT: hex-encoded unix timestamp"
  echo -e "       [-p <prefix>|--shoot-cluster-name-prefix <pf>]  Shoot cluster name prefix.
                                                       DEFAULT: ci"
  echo -e "       [--kubectl-path <path>]              Path to kubectl binary.
                                                       DEFAULT: kubectl (assumed \$PATH contains kubectl)"
  echo -e "       [--helm-path <path>]                 Path to helm binary.
                                                       DEFAULT: helm (assumed \$PATH contains helm)"
  echo -e "       [--wait-for-cluster-deletion] . . . . . . . . . Waits for garden-controller-manager to delete shoot cluster.
                                                       Required if garden-controller-manager is deleted after this script returns,
                                                       as done in ci-pipeline."
}

# Dispatch
function dispatch() {
  case $1 in
    "-h") help; exit 0;;
    "-c") cluster_only=true;   dispatch "${@:2}";;
    "--ci-cli") ci_cli="$(readlink -f "${2}")"; dispatch "${@:3}";;
    "-k"|"--garden-cluster-kubeconfig") garden_cluster_kubeconfig="$(readlink -f "$2")"; dispatch "${@:3}";;
    "--project-name"|"-n") watch_namespace="$2"; dispatch "${@:3}";;
    "--gardener-namespace") gardener_namespace="$2"; dispatch "${@:3}";;
    "--rc-on-cleanup-error") rc_cleanup_error="$2"; dispatch "${@:3}";;
    "--minimal-log-level") minimal_log_level="$2"; dispatch "${@:3}";;
    "--shoot-spec") shoot_spec="$2"; dispatch "${@:3}";;
    "--cloud-provider") cloud_provider="$2"; dispatch "${@:3}";;
    "--kubectl-path") kubectl_binary_path="$2" ;dispatch "${@:3}";;
    "--helm-path") helm_binary_path="$2" ;dispatch "${@:3}";;
    "-d"|"--domain-suffix") shoot_domain_suffix="$2"; dispatch "${@:3}";;
    "-s"|"--shoot-cluster-name-suffix") shoot_cluster_name_suffix="$2"; dispatch "${@:3}";;
    "-p"|"--shoot-cluster-name-prefix") shoot_cluster_name_prefix="$2"; dispatch "${@:3}";;
         "--wait-for-cluster-deletion") wait_for_cluster_deletion=true; dispatch "${@:2}";;
    *)    if [[ $# -ne 0 ]]; then fail "invalid command line arguments"; fi; execute;;
  esac
}

# Entry point
dispatch "${@}"